{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6492b-e9fc-4295-9194-a1b4adf94664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e827472-9a08-4fcc-8c36-0acfaf49d2d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813a908-9efa-47e1-a4fc-23cf29a17c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Step 1: Authenticate with Hugging Face\n",
    "# ----------------------------------------------------------\n",
    "#   1. Create an account at https://huggingface.co/\n",
    "#   2. Generate a Personal Access Token (PAT) from: Settings → Access Tokens\n",
    "#   3. Use that token to log in and enable model downloads.\n",
    "# login(token=\"PasteYourTokerHere\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa770e66-047a-429c-aa81-35bc3b4f34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Step 2: Select and download a Stable Diffusion model\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Choose a local directory where the downloaded model files\n",
    "local_dir = \"./stable-code-3b\"\n",
    "model_id = \"stabilityai/stable-code-3b\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,   # Use float16 if running on GPU\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Step 3: Save the downloaded model locally\n",
    "# ----------------------------------------------------------\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "model.save_pretrained(local_dir)\n",
    "\n",
    "print(f\"Model saved locally at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a71fe",
   "metadata": {},
   "source": [
    "## Loading model from Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f291dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\LLM\\Chat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc4fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:38<00:00, 12.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stable-Code-3B loaded in 4-bit\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define local model path\n",
    "local_model_dir = \"./stable-code-3b\"\n",
    "\n",
    "# Step 2: Quantization config \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Step 3: Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Step 4: Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"✅ Stable-Code-3B loaded in 4-bit\")\n",
    "print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98de03-248c-4557-93a6-9973527ddc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function that checks if a number is prime or not.\n",
      "\n",
      "# In[2]:\n",
      "\n",
      "\n",
      "def is_prime(number):\n",
      "    if number == 1:\n",
      "        return False\n",
      "    for i in range(2, number):\n",
      "        if number % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "\n",
      "# In[3]:\n",
      "\n",
      "\n",
      "is_prime(2)\n",
      "\n",
      "\n",
      "# In[4]:\n",
      "\n",
      "\n",
      "is_prime(3)\n",
      "\n",
      "\n",
      "# In[5]:\n",
      "\n",
      "\n",
      "is_prime(4)\n",
      "\n",
      "\n",
      "# In[6]:\n",
      "\n",
      "\n",
      "is_prime(5)\n",
      "\n",
      "\n",
      "# In[\n"
     ]
    }
   ],
   "source": [
    "# Provide a prompt for text generation\n",
    "prompt = '''Write a Python function that checks if a number is prime or not.'''\n",
    "\n",
    "# Tokenize the prompt and move inputs to the model device (GPU)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=120,   # length of generated text\n",
    "        temperature=0.2,      # lower = more deterministic\n",
    "        do_sample=True        # allow sampling / creativity\n",
    "    )\n",
    "\n",
    "# Decode and print the generated text\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7be9a",
   "metadata": {},
   "source": [
    "## Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9b87ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_prime(n):\n",
    "    if n == 1:\n",
    "        return False\n",
    "    for i in range(2, n):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "is_prime(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a6c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
